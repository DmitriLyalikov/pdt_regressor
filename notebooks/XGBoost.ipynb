{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost (Extreme Gradient Boosting)\n",
    "- uses regularization (add info to reduce variance and prevent overfitting)\n",
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Stratified fold includes the same percentage of target values in each fold\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " use the same folds to obtain new scores when fine-tuning hyperparameters.\n",
    " with GridSearchCV and RandomizedSearchCV\n",
    " GridsearchCV searches all possible combinations in a hyperparameter\n",
    " to find the best results. RandomizedSearchCV selects 10 random hyperparameters by default"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid_search(params, random=False):\n",
    "    # Initialize XGB Regressor with objective='reg:squarederror' (MSE)\n",
    "    xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2)\n",
    "    if random:\n",
    "        grid = RandomizedSearchCV(xgb, params, cv=kfold, n_iter=20, n_jobs=-1)\n",
    "    else:\n",
    "        grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1)\n",
    "    grid.fit()\n",
    "    best_params = grid.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = grid.best_score_\n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important XGBoost Hyperparamers:\n",
    "* n_estimators: default 100. (1..inf). (number of trees in ensembled)\n",
    "    - increasing may improve scores with large data\n",
    "* learning_rate: default 0.3 (0..inf). Shrinks the tree weights in each round of boosting\n",
    "    - Decreasing prevents overfitting\n",
    "* max_depth: default 6 (0..inf). Depth of the tree\n",
    "    - Decreasing prevents overfitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " n_estimators provides the number of tress in the ensemble\n",
    " initialize a grid search of n_estimators with default of 100\n",
    " Then double the numbers of trees through 800:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgrid_search\u001B[49m(params\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_estimators\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m400\u001B[39m, \u001B[38;5;241m800\u001B[39m]})\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Since we have a small dataset, increasing n_estimators did not produce\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# better results\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "grid_search(params={'n_estimators': [100, 200, 400, 800]})\n",
    "# Since we have a small dataset, increasing n_estimators did not produce\n",
    "# better results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " learning_rate shrinks the weights of trees for each round of boosting\n",
    " by lowering the learning rate, more trees are required to produce better scores\n",
    " This prevents overfitting because the size of the weights carried forward\n",
    " is smaller"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " max_depth determines the length of the tree, equivalent to the number\n",
    " of rounds of splitting. Limiting max depth prevents overfitting because\n",
    " the individual trees can only grow as far as max_depth allows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_search(params={'max_depth':[2, 3, 5, 6, 8]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " gamma: (lagrange multiplier) provides a threhold that nodes must surpass\n",
    " before making further splits according to the loss function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_search(params={'gamma':[0, 0.1, 0.5, 1, 2, 5]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Minimum child weight refers to the minimum sum of weights required for a node\n",
    " to split into a child. reduces overfitting by increasing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_search(params={'min_child_weight':[1, 2, 3, 4, 5]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying Early Stopping\n",
    "Early stopping limits the number of training rounds in iterative machine learning algorithms. it stops training when n consecutive rounds continue without producing gains."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "eval_set and eval_metric may be used as parameters for .fit to generate test scores for each training round. eval metric provides the scoring method, 'rmse' for regression. eval_set provides the test to be evaluated, commonly X_test and y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# eval_set = [(X_test, y_test)]\n",
    "# eval_metric = 'rmse'\n",
    "# xgb.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
