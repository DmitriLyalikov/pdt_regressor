{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Pendant Drop Tensiometry Regression Models\n",
    "#### Algorithms\n",
    "* XGBoost\n",
    "* LightGBM\n",
    "#### Data:\n",
    "* This model takes the labeled set of features of the pendant drop profile and becomes a function of beta.\n",
    "* Input features include Drop Height, Capillary Radius, R-s, R-e, and Smax.\n",
    "\n",
    "\n",
    "The current model is trained, tested, and tuned on dataset (data/pdt-dataset.csv) which has 2500 entries.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "   Drop Height  Capillary Radius     R-s      R-e     Smax    Beta\n0      2931984            675340  890067  1085611  3590000  400000\n1      3041870            673539  885918  1094985  3689763  400000\n2      3131424            666058  889661  1085394  3789526  400000\n3      3235418            674748  884866  1085809  3889289  400000\n4      3335846            689308  896088  1086557  3989053  400000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Drop Height</th>\n      <th>Capillary Radius</th>\n      <th>R-s</th>\n      <th>R-e</th>\n      <th>Smax</th>\n      <th>Beta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2931984</td>\n      <td>675340</td>\n      <td>890067</td>\n      <td>1085611</td>\n      <td>3590000</td>\n      <td>400000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3041870</td>\n      <td>673539</td>\n      <td>885918</td>\n      <td>1094985</td>\n      <td>3689763</td>\n      <td>400000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3131424</td>\n      <td>666058</td>\n      <td>889661</td>\n      <td>1085394</td>\n      <td>3789526</td>\n      <td>400000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3235418</td>\n      <td>674748</td>\n      <td>884866</td>\n      <td>1085809</td>\n      <td>3889289</td>\n      <td>400000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3335846</td>\n      <td>689308</td>\n      <td>896088</td>\n      <td>1086557</td>\n      <td>3989053</td>\n      <td>400000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am multiplying all elements by 10^6, to keep float integrity when using gridsearchCV as int64\n",
    "df = pd.read_csv('../data/pdt-dataset.csv').apply(lambda x: x*1000000).astype('int64')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0    400000\n1    400000\n2    400000\n3    400000\n4    400000\nName: Beta, dtype: int64"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model predicts Beta given a Pendant Drop Profile\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "\n",
    "# Stratified fold includes the same percentage of target values in each fold.\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "y.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# This function takes a list of hyperparameter configs and finds the best one.\n",
    "def grid_search(params, random=False):\n",
    "    # Initialize XGB Regressor with objective='reg:squarederror' (MSE)\n",
    "    xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2)\n",
    "    if random:\n",
    "        grid = RandomizedSearchCV(xgb, params, cv=kfold, n_iter=20, n_jobs=-1)\n",
    "    else:\n",
    "        grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    best_params = grid.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = grid.best_score_\n",
    "    print(\"Training score: {:.3f}\".format(best_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_estimators': 800}\n",
      "Training score: 0.999\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'n_estimators': [100, 200, 400, 800]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.1}\n",
      "Training score: 0.999\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 8}\n",
      "Training score: 0.999\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'max_depth':[2, 3, 5, 6, 8]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuned XGBoost Regressor\n",
    "* n-estimators: 800\n",
    "* learning_rate=.1\n",
    "* max_depth = 5\n",
    "\n",
    "Accuracy score on test data (.999), RMSE: (0.0034324513493428823)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.003500566413769064\n",
      "Normalized Mean 0.005834277356281772\n"
     ]
    }
   ],
   "source": [
    "# Build, train, test, and save our model\n",
    "xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2, learning_rate=.1, n_estimators=800, max_depth=5)\n",
    "\n",
    "df = pd.read_csv('../data/pdt-dataset.csv')\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "\n",
    "print(f\"RMSE: {reg_rmse}\")\n",
    "# print(lgbm_reg.feature_importances_)\n",
    "norm_mean = reg_rmse / np.mean(y)\n",
    "print(f\"Normalized Mean {norm_mean}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save our previous model to models folder\n",
    "with open(\"../models/pdt-regression-model.pkl\", 'wb') as f:\n",
    "    pickle.dump(xgb, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An example of how to use saved models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the model from models folder\n",
    "with open(\"../models/pdt-regression-model.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experimenting with wider beta range data set (.1-.8) on same model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004374367554479346\n",
      "RMSE: 0.00474501474542218\n",
      "Normalized Mean 0.007908357909036965\n"
     ]
    }
   ],
   "source": [
    "# Build, train, test, and save our model\n",
    "xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2, learning_rate=.1, n_estimators=800, max_depth=5)\n",
    "\n",
    "df = pd.read_csv('../data/pdt-dataset-wider-beta.csv')\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "print(reg_rmse)\n",
    "\n",
    "\n",
    "# let's test on original data\n",
    "df = pd.read_csv('../data/pdt-dataset.csv')\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "print(f\"RMSE: {reg_rmse}\")\n",
    "# print(lgbm_reg.feature_importances_)\n",
    "norm_mean = reg_rmse / np.mean(y)\n",
    "print(f\"Normalized Mean {norm_mean}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiment with same model but without Smax as training data and larger range of beta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.004334942946277289\n",
      "Normalized Mean 0.009633206547282863\n"
     ]
    }
   ],
   "source": [
    "# Build, train, test, and save our model\n",
    "xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2, learning_rate=.1, n_estimators=800, max_depth=5)\n",
    "\n",
    "df = pd.read_csv('../data/pdt-dataset-wider-beta-no-Smax.csv')\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "print(f\"RMSE: {reg_rmse}\")\n",
    "# print(lgbm_reg.feature_importances_)\n",
    "norm_mean = reg_rmse / np.mean(y)\n",
    "print(f\"Normalized Mean {norm_mean}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build and Characterize a LightGBM Regressor Model for the same 3 datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   Drop Height  Capillary Radius     R-s      R-e    Beta\n0      2305560             98640  469920  1017619  100000\n1      2322946             83864  460409  1017719  100000\n2      2347761             78837  440249  1017397  100000\n3      2372782             81146  454684  1019378  100000\n4      2398734             91546  461319  1017129  100000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Drop Height</th>\n      <th>Capillary Radius</th>\n      <th>R-s</th>\n      <th>R-e</th>\n      <th>Beta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2305560</td>\n      <td>98640</td>\n      <td>469920</td>\n      <td>1017619</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2322946</td>\n      <td>83864</td>\n      <td>460409</td>\n      <td>1017719</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2347761</td>\n      <td>78837</td>\n      <td>440249</td>\n      <td>1017397</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2372782</td>\n      <td>81146</td>\n      <td>454684</td>\n      <td>1019378</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2398734</td>\n      <td>91546</td>\n      <td>461319</td>\n      <td>1017129</td>\n      <td>100000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/pdt-dataset-wider-beta-no-Smax.csv').apply(lambda x: x*1000000).astype('int64')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0    100000\n1    100000\n2    100000\n3    100000\n4    100000\nName: Beta, dtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model predicts Beta given a Pendant Drop Profile\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "\n",
    "# Stratified fold includes the same percentage of target values in each fold.\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "y.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# This function takes a list of hyperparameter configs and finds the best one.\n",
    "def lgb_grid_search(params, random=False):\n",
    "    # Initialize LightGBM Regressor with objective='reg:squarederror' (MSE)\n",
    "    lgb_reg = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression',\n",
    "    random_state=2)\n",
    "    if random:\n",
    "        grid = RandomizedSearchCV(lgb_reg, params, cv=kfold, n_iter=20, n_jobs=-1)\n",
    "    else:\n",
    "        grid = GridSearchCV(lgb_reg, params, cv=kfold, n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    best_params = grid.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = grid.best_score_\n",
    "    print(\"Training score: {:.3f}\".format(best_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.1, 'max_depth': 5}\n",
      "Training score: 1.000\n"
     ]
    }
   ],
   "source": [
    "param_list = [\n",
    "    {'learning_rate': [0.1, 0.01], 'max_depth': [3, 5]},\n",
    "    {'learning_rate': [0.05, 0.01], 'max_depth': [4, 6]}\n",
    "]\n",
    "best_regressor = lgb_grid_search(param_list, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'early_stopping_rounds': None, 'learning_rate': 0.1, 'max_depth': 20, 'min_child_samples': 10, 'n_estimators': 250, 'num_leaves': 50}\n",
      "Training score: 1.000\n"
     ]
    }
   ],
   "source": [
    "param_list = {\n",
    "    'num_leaves': [10, 20, 50],  # Maximum number of leaves in a tree\n",
    "    'max_depth': [5, 10, 20],  # Maximum depth of a tree\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate for gradient boosting\n",
    "    'n_estimators': [50, 100, 250],  # Number of trees to fit\n",
    "    # 'subsample_for_bin': [20000, 50000, 100000],  # Number of samples for constructing bins\n",
    "    'min_child_samples': [5, 10, 20],  # Minimum number of samples required to form a leaf node\n",
    "    #'reg_alpha': [0, 0.1, 0.5, 1],  # L1 regularization\n",
    "    #'reg_lambda': [0, 0.1, 0.5, 1],  # L2 regularization\n",
    "    #'colsample_bytree': [0.5, 0.7, 1.0],  # Fraction of features to consider at each split\n",
    "    #'subsample': [0.5, 0.7, 1.0],  # Fraction of samples to use for each tree\n",
    "    #'min_split_gain': [0, 0.1, 0.5],  # Minimum loss reduction required to form a new split\n",
    "    #'max_bin': [255, 511, 1023],  # Maximum number of bins to use\n",
    "    'early_stopping_rounds': [None, 5, 10, 15],  # Early stopping based on the validation set score\n",
    "}\n",
    "best_regressor = lgb_grid_search(param_list, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuned LightGBM Model\n",
    "* early_stopping_rounds=None\n",
    "* learnging_rate=0.1\n",
    "* max_depth=20\n",
    "* min_child_samples=10\n",
    "* n_estimators=250\n",
    "* num_leaves=50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.004259963266041081\n",
      "Normalized Mean 0.009466585035646846\n"
     ]
    }
   ],
   "source": [
    "# Build, train, test, and save our LightGBM model\n",
    "lgbm_reg = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression',\n",
    "                            early_stopping_rounds=None,\n",
    "                            learning_rate=0.1,\n",
    "                            max_depth=20,\n",
    "                            min_child_samples=10,\n",
    "                            n_estimators=250,\n",
    "                            num_leaves=50,\n",
    "                            random_state=2)\n",
    "\n",
    "df = pd.read_csv('../data/pdt-dataset-wider-beta-no-Smax.csv')\n",
    "\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lgbm_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lgbm_reg.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "print(f\"RMSE: {reg_rmse}\")\n",
    "# print(lgbm_reg.feature_importances_)\n",
    "norm_mean = reg_rmse / np.mean(y)\n",
    "print(f\"Normalized Mean {norm_mean}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the custom scoring function that calculates normalized MSE\n",
    "def normalized_mse(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    norm_factor = np.mean(y_true)\n",
    "    normalized_mse = mse / norm_factor**2\n",
    "    return -normalized_mse   # return negative value to minimize the metric\n",
    "\n",
    "# We can now start a grid search with our custom scoring function\n",
    "grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, scoring=make_scorer(normalized_mse, greater_is_better=False), cv=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
