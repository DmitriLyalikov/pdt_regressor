{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Pendant Drop Tensiometry Regression Models\n",
    "#### Algorithms\n",
    "* XGBoost\n",
    "* LightGBM\n",
    "#### Data:\n",
    "* This model takes the labeled set of features of the pendant drop profile and becomes a function of beta.\n",
    "* Input features include Drop Height, Capillary Radius, R-s, R-e, and Smax.\n",
    "\n",
    "\n",
    "The current model is trained, tested, and tuned on dataset (data/pdt-dataset.csv) which has 2500 entries.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   Drop Height  Capillary Radius     R-s      R-e    Beta\n0      2296692             93178  469050  1016784  100000\n1      2315458             91732  469674  1013074  100000\n2      2325057             85606  459445  1014251  100000\n3      2340761             84406  452211  1017714  100000\n4      2348701             80062  457289  1019234  100000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Drop Height</th>\n      <th>Capillary Radius</th>\n      <th>R-s</th>\n      <th>R-e</th>\n      <th>Beta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2296692</td>\n      <td>93178</td>\n      <td>469050</td>\n      <td>1016784</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2315458</td>\n      <td>91732</td>\n      <td>469674</td>\n      <td>1013074</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2325057</td>\n      <td>85606</td>\n      <td>459445</td>\n      <td>1014251</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2340761</td>\n      <td>84406</td>\n      <td>452211</td>\n      <td>1017714</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2348701</td>\n      <td>80062</td>\n      <td>457289</td>\n      <td>1019234</td>\n      <td>100000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am multiplying all elements by 10^6, to keep float integrity when using gridsearchCV as int64\n",
    "df = pd.read_csv('../data/pdt-training-set.csv').apply(lambda x: x*1000000).astype('int64')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0    100000\n1    100000\n2    100000\n3    100000\n4    100000\nName: Beta, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model predicts Beta given a Pendant Drop Profile\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "\n",
    "# Stratified fold includes the same percentage of target values in each fold.\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "y.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# This function takes a list of hyperparameter configs and finds the best one.\n",
    "def grid_search(params, random=False):\n",
    "    # Initialize XGB Regressor with objective='reg:squarederror' (MSE)\n",
    "    xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2)\n",
    "    if random:\n",
    "        grid = RandomizedSearchCV(xgb, params, cv=kfold, n_iter=20, n_jobs=-1)\n",
    "    else:\n",
    "        grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    best_params = grid.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = grid.best_score_\n",
    "    print(\"Training score: {:.3f}\".format(best_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "param_list = {\n",
    "    'max_depth': [20, 30],  # Maximum depth of a tree\n",
    "    'n_estimators' : [800],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],  # Learning rate for gradient boosting\n",
    "    #'n_estimators': [400, 500, 700, 800],  # Number of trees to fit\n",
    "    # 'subsample_for_bin': [20000, 50000, 100000],  # Number of samples for constructing bins\n",
    "    #'gamma' :  [0, 1, 5, 10]\n",
    "\n",
    "}\n",
    "grid_search(params=param_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.1}\n",
      "Training score: 0.999\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 8}\n",
      "Training score: 0.999\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'max_depth':[2, 3, 5, 6, 8]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuned XGBoost Regressor\n",
    "* n-estimators: 800\n",
    "* learning_rate=.1\n",
    "* max_depth = 5\n",
    "\n",
    "Accuracy score on test data (.999), RMSE: (0.0034324513493428823)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0031808014766063542\n",
      "Normalized Mean 0.007068447725791898\n"
     ]
    }
   ],
   "source": [
    "# Build, train, test, and save our model\n",
    "xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2, learning_rate=.1, n_estimators=800, max_depth=5)\n",
    "\n",
    "df = pd.read_csv('../data/pdt-training-set.csv')\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "\n",
    "print(f\"RMSE: {reg_rmse}\")\n",
    "# print(lgbm_reg.feature_importances_)\n",
    "norm_mean = reg_rmse / np.mean(y)\n",
    "print(f\"Normalized Mean {norm_mean}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Save our previous model to models folder\n",
    "with open(\"../models/pdt-regression-model.pkl\", 'wb') as f:\n",
    "    pickle.dump(xgb, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An example of how to use saved models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the model from models folder\n",
    "with open(\"../models/pdt-regression-model.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experimenting with wider beta range data set (.1-.8) on same model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028797898852983487\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 4, got 5",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 27\u001B[0m\n\u001B[0;32m     23\u001B[0m y \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBeta\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     24\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[0;32m     25\u001B[0m X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 27\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mxgb_wide\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m reg_mse \u001B[38;5;241m=\u001B[39m mean_squared_error(y_test, y_pred)\n\u001B[0;32m     30\u001B[0m reg_rmse \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(reg_mse)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdt_regressor\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1114\u001B[0m, in \u001B[0;36mXGBModel.predict\u001B[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001B[0m\n\u001B[0;32m   1112\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_can_use_inplace_predict():\n\u001B[0;32m   1113\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1114\u001B[0m         predts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_booster\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace_predict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1115\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1116\u001B[0m \u001B[43m            \u001B[49m\u001B[43miteration_range\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43miteration_range\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1117\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpredict_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmargin\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput_margin\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1118\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmissing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1119\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbase_margin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_margin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1120\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalidate_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1121\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1122\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m _is_cupy_array(predts):\n\u001B[0;32m   1123\u001B[0m             \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcupy\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=import-error\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdt_regressor\\venv\\lib\\site-packages\\xgboost\\core.py:2269\u001B[0m, in \u001B[0;36mBooster.inplace_predict\u001B[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001B[0m\n\u001B[0;32m   2265\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m   2266\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`shape` attribute is required when `validate_features` is True.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2267\u001B[0m         )\n\u001B[0;32m   2268\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_features() \u001B[38;5;241m!=\u001B[39m data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m-> 2269\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2270\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature shape mismatch, expected: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_features()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2271\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgot \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2272\u001B[0m         )\n\u001B[0;32m   2274\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m   2275\u001B[0m     _array_interface,\n\u001B[0;32m   2276\u001B[0m     _is_cudf_df,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2279\u001B[0m     _transform_pandas_df,\n\u001B[0;32m   2280\u001B[0m )\n\u001B[0;32m   2282\u001B[0m enable_categorical \u001B[38;5;241m=\u001B[39m _has_categorical(\u001B[38;5;28mself\u001B[39m, data)\n",
      "\u001B[1;31mValueError\u001B[0m: Feature shape mismatch, expected: 4, got 5"
     ]
    }
   ],
   "source": [
    "# Build, train, test, and save our model\n",
    "xgb_wide = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2, learning_rate=.01, n_estimators=800, max_depth=30, gamma=0)\n",
    "\n",
    "df = pd.read_csv('../data/pdt-training-set.csv')\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "xgb_wide.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_wide.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "print(reg_rmse)\n",
    "\n",
    "\n",
    "# let's test on original data\n",
    "df = pd.read_csv('../data/pdt-dataset.csv')\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_pred = xgb_wide.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "print(f\"RMSE: {reg_rmse}\")\n",
    "# print(lgbm_reg.feature_importances_)\n",
    "norm_mean = reg_rmse / np.mean(y)\n",
    "print(f\"Normalized Mean {norm_mean}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiment with same model but without Smax as training data and larger range of beta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build, train, test, and save our model\n",
    "xgb_wide = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
    "    random_state=2, learning_rate=.1, n_estimators=800, max_depth=5)\n",
    "\n",
    "df = pd.read_csv('../data/pdt-training-set-large.csv')\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "xgb_wide.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_wide.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "print(f\"RMSE: {reg_rmse}\")\n",
    "# print(lgbm_reg.feature_importances_)\n",
    "norm_mean = reg_rmse / np.mean(y)\n",
    "print(f\"Normalized Mean {norm_mean}\")\n",
    "\n",
    "# Save our previous model to models folder\n",
    "with open(\"../models/xgboost-wide-beta-semituned-model.pkl\", 'wb') as f:\n",
    "    pickle.dump(xgb_wide, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build and Characterize a LightGBM Regressor Model for the same 3 datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   Drop Height  Capillary Radius     R-s      R-e    Beta\n0      2305560             98640  469920  1017619  100000\n1      2322946             83864  460409  1017719  100000\n2      2347761             78837  440249  1017397  100000\n3      2372782             81146  454684  1019378  100000\n4      2398734             91546  461319  1017129  100000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Drop Height</th>\n      <th>Capillary Radius</th>\n      <th>R-s</th>\n      <th>R-e</th>\n      <th>Beta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2305560</td>\n      <td>98640</td>\n      <td>469920</td>\n      <td>1017619</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2322946</td>\n      <td>83864</td>\n      <td>460409</td>\n      <td>1017719</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2347761</td>\n      <td>78837</td>\n      <td>440249</td>\n      <td>1017397</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2372782</td>\n      <td>81146</td>\n      <td>454684</td>\n      <td>1019378</td>\n      <td>100000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2398734</td>\n      <td>91546</td>\n      <td>461319</td>\n      <td>1017129</td>\n      <td>100000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/pdt-dataset-wider-beta-no-Smax.csv').apply(lambda x: x*1000000).astype('int64')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0    100000\n1    100000\n2    100000\n3    100000\n4    100000\nName: Beta, dtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model predicts Beta given a Pendant Drop Profile\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "\n",
    "# Stratified fold includes the same percentage of target values in each fold.\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "y.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# This function takes a list of hyperparameter configs and finds the best one.\n",
    "def lgb_grid_search(params, random=False):\n",
    "    # Initialize LightGBM Regressor with objective='reg:squarederror' (MSE)\n",
    "    lgb_reg = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression',\n",
    "    random_state=2)\n",
    "    if random:\n",
    "        grid = RandomizedSearchCV(lgb_reg, params, cv=kfold, n_iter=20, n_jobs=-1)\n",
    "    else:\n",
    "        grid = GridSearchCV(lgb_reg, params, cv=kfold, n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    best_params = grid.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_score = grid.best_score_\n",
    "    print(\"Training score: {:.3f}\".format(best_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.1, 'max_depth': 5}\n",
      "Training score: 1.000\n"
     ]
    }
   ],
   "source": [
    "param_list = [\n",
    "    {'learning_rate': [0.1, 0.01], 'max_depth': [3, 5]},\n",
    "    {'learning_rate': [0.05, 0.01], 'max_depth': [4, 6]}\n",
    "]\n",
    "best_regressor = lgb_grid_search(param_list, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'early_stopping_rounds': None, 'learning_rate': 0.1, 'max_depth': 20, 'min_child_samples': 10, 'n_estimators': 250, 'num_leaves': 50}\n",
      "Training score: 1.000\n"
     ]
    }
   ],
   "source": [
    "param_list = {\n",
    "    'num_leaves': [10, 20, 50],  # Maximum number of leaves in a tree\n",
    "    'max_depth': [5, 10, 20],  # Maximum depth of a tree\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate for gradient boosting\n",
    "    'n_estimators': [50, 100, 250],  # Number of trees to fit\n",
    "    # 'subsample_for_bin': [20000, 50000, 100000],  # Number of samples for constructing bins\n",
    "    'min_child_samples': [5, 10, 20],  # Minimum number of samples required to form a leaf node\n",
    "    #'reg_alpha': [0, 0.1, 0.5, 1],  # L1 regularization\n",
    "    #'reg_lambda': [0, 0.1, 0.5, 1],  # L2 regularization\n",
    "    #'colsample_bytree': [0.5, 0.7, 1.0],  # Fraction of features to consider at each split\n",
    "    #'subsample': [0.5, 0.7, 1.0],  # Fraction of samples to use for each tree\n",
    "    #'min_split_gain': [0, 0.1, 0.5],  # Minimum loss reduction required to form a new split\n",
    "    #'max_bin': [255, 511, 1023],  # Maximum number of bins to use\n",
    "    'early_stopping_rounds': [None, 5, 10, 15],  # Early stopping based on the validation set score\n",
    "}\n",
    "best_regressor = lgb_grid_search(param_list, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuned LightGBM Model\n",
    "* early_stopping_rounds=None\n",
    "* learnging_rate=0.1\n",
    "* max_depth=20\n",
    "* min_child_samples=10\n",
    "* n_estimators=250\n",
    "* num_leaves=50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.003381777893112043\n",
      "Normalized Mean 0.0075150619846934275\n"
     ]
    }
   ],
   "source": [
    "# Build, train, test, and save our LightGBM model\n",
    "lgbm_reg = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression',\n",
    "                            early_stopping_rounds=None,\n",
    "                            learning_rate=0.101,\n",
    "                            max_depth=12,\n",
    "                            min_child_samples=10,\n",
    "                            n_estimators=250,\n",
    "                            num_leaves=50,\n",
    "                            random_state=2)\n",
    "\n",
    "df = pd.read_csv('../data/pdt-training-set-large.csv')\n",
    "\n",
    "X = df.drop('Beta', axis=1)\n",
    "y = df['Beta']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lgbm_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lgbm_reg.predict(X_test)\n",
    "\n",
    "reg_mse = mean_squared_error(y_test, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "print(f\"RMSE: {reg_rmse}\")\n",
    "# print(lgbm_reg.feature_importances_)\n",
    "norm_mean = reg_rmse / np.mean(y)\n",
    "print(f\"Normalized Mean {norm_mean}\")\n",
    "\n",
    "# Save our previous model to models folder\n",
    "with open(\"../models/lightgbm-wide-beta-tuned-model.pkl\", 'wb') as f:\n",
    "    pickle.dump(lgbm_reg, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Prediction: [0.5259061]\n",
      "LightGBM Prediction: [0.50138153]\n"
     ]
    }
   ],
   "source": [
    "features_sample = pd.read_csv(\"../data/Feature Sets/features.csv\")\n",
    "\n",
    "features = features_sample.values\n",
    "xgb_beta  = xgb_wide.predict(features)\n",
    "lgbm_beta = lgbm_reg.predict(features)\n",
    "\n",
    "print(f\"XGBoost Prediction: {xgb_beta}\")\n",
    "print(f\"LightGBM Prediction: {lgbm_beta}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5111865  0.5111865  0.51420826 0.51938087 0.5284546  0.5024724\n",
      " 0.51394206 0.50076514 0.511241   0.50398254 0.51095444 0.5060648\n",
      " 0.53052175 0.5284537 ]\n",
      "[0.48035381 0.48116316 0.48387299 0.48929479 0.49619707 0.47027886\n",
      " 0.48462418 0.45514608 0.48116316 0.4735255  0.4784555  0.4735255\n",
      " 0.50016715 0.49619707]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/test/matlab-canny-features.csv')\n",
    "features = df.drop('image', axis=1)\n",
    "images = df['image']\n",
    "\n",
    "features = features.values\n",
    "xgb_beta  = xgb_wide.predict(features)\n",
    "lgbm_beta = lgbm_reg.predict(features)\n",
    "# Create empty columns for predictions in the dataset\n",
    "#df['XGBoost_Prediction'] = ''\n",
    "#df['LightGBM_Prediction'] = ''\n",
    "print(xgb_beta)\n",
    "print(lgbm_beta)\n",
    "\n",
    "df['XGBoost Prediction'] = xgb_beta\n",
    "df['LightGBM Prediction'] = lgbm_beta\n",
    "\n",
    "df.to_csv('../data/test/matlab-canny-features-predictions.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
